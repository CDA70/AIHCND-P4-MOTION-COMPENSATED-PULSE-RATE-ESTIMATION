{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulse Rate Algorithm\n",
    "\n",
    "### Contents\n",
    "Fill out this notebook as part of your final project submission.\n",
    "\n",
    "**You will have to complete both the Code and Project Write-up sections.**\n",
    "- The [Code](#Code) is where you will write a **pulse rate algorithm** and already includes the starter code.\n",
    "   - Imports - These are the imports needed for Part 1 of the final project. \n",
    "     - [glob](https://docs.python.org/3/library/glob.html)\n",
    "     - [numpy](https://numpy.org/)\n",
    "     - [scipy](https://www.scipy.org/)\n",
    "- The [Project Write-up](#Project-Write-up) to describe why you wrote the algorithm for the specific case.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "You will be using the **Troika**[1] dataset to build your algorithm. Find the dataset under `datasets/troika/training_data`. The `README` in that folder will tell you how to interpret the data. The starter code contains a function to help load these files.\n",
    "\n",
    "1. Zhilin Zhang, Zhouyue Pi, Benyuan Liu, ‘‘TROIKA: A General Framework for Heart Rate Monitoring Using Wrist-Type Photoplethysmographic Signals During Intensive Physical Exercise,’’IEEE Trans. on Biomedical Engineering, vol. 62, no. 2, pp. 522-531, February 2015. Link\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n    window_length = 8 * fs\\n    window_shift = 2 * fs\\n\\n    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\\n\\n    # Load the reference data, also named as \"ground truth\" heart rate data\\n    reference_bpm = sp.io.loadmat(ref_fl)[\\'BPM0\\']\\n\\n    # filter the data using bandpass\\n    ppg_bandpass  = BandpassFilter(ppg)\\n    accx_bandpass = BandpassFilter(accx)\\n    accy_bandpass = BandpassFilter(accy)\\n    accz_bandpass = BandpassFilter(accz)\\n\\n    # Calculate the acc magnitude\\n    acc_magnitude = np.sqrt(np.square(accx_bandpass) + np.square(accy_bandpass) + np.square(accz_bandpass))\\n    \\n    estimate = []\\n    confidence = []\\n    \\n    for i in range(0, len(ppg_bandpass) - window_length, window_shift):\\n        ppg_freqs, ppg_fft, ppg_weight = Features(ppg_bandpass[i:i+window_length], fs)\\n        acc_freqs, acc_fft, acc_weigth = Features(acc_magnitude[i:i+window_length], fs)\\n\\n        max_ppg = ppg_freqs[np.argmax(ppg_weight)]\\n        max_acc = acc_freqs[np.argmax(acc_weigth)]\\n            \\n        j = 1        \\n        while np.abs(max_ppg - max_acc) <= 0 and j <=2:\\n            j += 1\\n            max_ppg = ppg_freqs[np.argsort(ppg_fft, axis = 0)[-j]]\\n           \\n        estimate_frequency = max_ppg \\n        estimate.append(estimate_frequency * 60)\\n        \\n        window_Hz = 40/60\\n        conf = np.sum(ppg_fft[(ppg_freqs >= (estimate_frequency - window_Hz)) & \\n                        (ppg_freqs <= (estimate_frequency + window_Hz))]) / np.sum(ppg_fft)\\n        \\n        confidence.append(conf)\\n        \\n    errors = np.abs(np.diag(estimate - reference_bpm))\\n    \\n\\n    return errors, np.array(confidence)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    return data_fls, ref_fls\n",
    "\n",
    "\n",
    "def LoadTroikaDataFile(data_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    return data[2:]\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability.\n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and\n",
    "        corresponding reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse\n",
    "        rate error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))    \n",
    "\n",
    "def Evaluate():\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an\n",
    "    aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        # Run the pulse rate algorithm on each trial in the dataset\n",
    "        errors, confidence = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(errors)\n",
    "        confs.append(confidence)\n",
    "        # Compute aggregate error metric\n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "    return (AggregateErrorMetric(errs, confs), errs, confs)    \n",
    "\n",
    "def BandpassFilter(signal, bandpass=(40/60,240/60), fs=125):\n",
    "    \"\"\"\n",
    "    Bandpass Filter:\n",
    "        Filers the input signal, in this case between 40 and 240 BPM\n",
    "\n",
    "    Args:\n",
    "        signal:     (np.array) The input signal\n",
    "        pass_band:  (tuple) The pass band. Frequency components outside the bandpass are filered.\n",
    "        fs:         (number) The sampling rate of <signal>\n",
    "        \n",
    "    Returns:\n",
    "        (np.array) The filtered signal\n",
    "    \"\"\"\n",
    "    b, a = sp.signal.butter(3, bandpass, btype='bandpass', fs=fs)\n",
    "    return sp.signal.filtfilt(b, a, signal)\n",
    "\n",
    "\n",
    "def Features(signal, fs=125):\n",
    "    \"\"\" \n",
    "    Features:\n",
    "        A featurization of the signal data\n",
    "    Ags:\n",
    "        signal:     (np.array) The input signal\n",
    "        fs:         (number) The sampling rate of <signal>\n",
    "    returns:\n",
    "        list of frequencies, fft and weights\n",
    "    \"\"\"\n",
    "\n",
    "    # run the descrete Fourier Transform. it returns a float array\n",
    "    freqs = np.fft.rfftfreq(len(signal), 1 / fs)\n",
    "\n",
    "    # Take an FFT of the centered signal\n",
    "    fft = np.abs(np.fft.rfft(signal - np.mean(signal), len(signal)))\n",
    "    # filter the range between the lower and higher BPM\n",
    "    fft[freqs <= (40 / 60)] = 0.0\n",
    "    fft[freqs >= (240 / 60)] = 0.0\n",
    "\n",
    "    # the weight of the signal\n",
    "    weight = np.sum(np.square(signal - np.mean(signal)))\n",
    "\n",
    "    return freqs, fft, weight\n",
    "\n",
    "def EstimateMaxPPG(ppg_weight, acc_weight, ppg_frequencies, acc_frequencies, ppg_fft, acc_fft):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    max_ppg = ppg_frequencies[np.argmax(ppg_weight)]\n",
    "    max_acc = acc_frequencies[np.argmax(acc_weight)]\n",
    "\n",
    "    lcv = 1       \n",
    "    while np.abs(max_ppg - max_acc) <= 0 and lcv <=2:\n",
    "        lcv += 1\n",
    "        max_ppg = ppg_frequencies[np.argsort(ppg_fft, axis = 0)[-lcv]]\n",
    "        max_acc = acc_frequencies[np.argsort(acc_fft, axis = 0)[-lcv]]\n",
    "    \n",
    "    return max_ppg, max_acc\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    # Load data using LoadTroikaDataFile\n",
    "    ppg, accx, accy, accz = LoadTroikaDataFile(data_fl)\n",
    "    \n",
    "    # Load the reference data, also named as \"ground truth\" heart rate data\n",
    "    reference_bpm = sp.io.loadmat(ref_fl)['BPM0']\n",
    "    \n",
    "    # filter signals\n",
    "    ppg  = BandpassFilter(ppg)\n",
    "    accx = BandpassFilter(accx)\n",
    "    accy = BandpassFilter(accy)\n",
    "    accz = BandpassFilter(accz)\n",
    "    \n",
    "    # Calculate the acc magnitude\n",
    "    acc_magnitude = np.sqrt(np.square(accx) + np.square(accy) + np.square(accz))\n",
    "    \n",
    "    estimates = []\n",
    "    confidences = []\n",
    "    fs = 125\n",
    "    window_length = 8 * fs\n",
    "    window_shift = 2 * fs\n",
    "   \n",
    "    for i in range(0, len(ppg) - window_length, window_shift):\n",
    "        \n",
    "        ppg_freqs, ppg_fft, ppg_weigth = Features(ppg[i:i + window_length], fs)\n",
    "        acc_freqs, acc_fft, acc_weigth = Features(acc_magnitude[i:i + window_length], fs)\n",
    "\n",
    "        est_freqs = EstimateMaxPPG(ppg_weigth, acc_weigth, ppg_freqs, acc_freqs, ppg_fft, acc_fft)[0]  \n",
    "        estimates.append(est_freqs * 60)\n",
    "        \n",
    "        # calculate the confidence with a windows frequency of 40/60 (Hz)\n",
    "        window_f = 40/60\n",
    "        confidence = np.sum(ppg_fft[(ppg_freqs >= (est_freqs - window_f)) & (ppg_freqs <= (est_freqs + window_f))])/ np.sum(ppg_fft)                \n",
    "        confidences.append(confidence) \n",
    "        \n",
    "        # Return MEA and confidence.\n",
    "    errors = np.abs(np.diag(estimates - reference_bpm))\n",
    "    return np.array(errors), np.array(confidences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19.24238928158741,\n",
       " array([71.91079295,  2.39253394,  2.14285714, ...,  2.5       ,\n",
       "         1.6904    ,  3.4959    ]),\n",
       " array([0.5890318 , 0.59509154, 0.5531311 , ..., 0.72588548, 0.70679039,\n",
       "        0.73644751]))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "Evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['./datasets/troika/training_data\\\\DATA_01_TYPE01.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_02_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_03_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_04_TYPE01.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_04_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_05_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_06_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_07_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_08_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_10_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_11_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\DATA_12_TYPE02.mat'],\n",
       " ['./datasets/troika/training_data\\\\REF_01_TYPE01.mat',\n",
       "  './datasets/troika/training_data\\\\REF_02_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_03_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_04_TYPE01.mat',\n",
       "  './datasets/troika/training_data\\\\REF_04_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_05_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_06_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_07_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_08_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_10_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_11_TYPE02.mat',\n",
       "  './datasets/troika/training_data\\\\REF_12_TYPE02.mat'])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "data_fls, ref_fls = LoadTroikaDataset()\n",
    "data_fls, ref_fls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 8 * fs\n",
    "window_shift = 2 * fs\n",
    "\n",
    "ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "\n",
    "# Load the reference data, also named as \"ground truth\" heart rate data\n",
    "reference_bpm = sp.io.loadmat(ref_fls[0])['BPM0']\n",
    "\n",
    "# filter the data using bandpass\n",
    "ppg_bandpass  = BandpassFilter(ppg)\n",
    "accx_bandpass = BandpassFilter(accx)\n",
    "accy_bandpass = BandpassFilter(accy)\n",
    "accz_bandpass = BandpassFilter(accz)\n",
    "\n",
    "# Calculate the acc magnitude\n",
    "acc_magnitude = np.sqrt(np.square(accx_bandpass) + np.square(accy_bandpass) + np.square(accz_bandpass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "estimate = []\n",
    "confidence = []\n",
    "    \n",
    "for i in range(0, len(ppg_bandpass) - window_length, window_shift):\n",
    "    ppg_freqs, ppg_fft, ppg_weight = Features(ppg_bandpass[i:i+window_length], fs)\n",
    "    acc_freqs, acc_fft, acc_weight = Features(acc_magnitude[i:i+window_length], fs)\n",
    "    \n",
    "    max_ppg = ppg_freqs[np.argmax(ppg_weight)]\n",
    "    max_acc = acc_freqs[np.argmax(acc_weight)]\n",
    "            \n",
    "    j = 1        \n",
    "    while np.abs(max_ppg - max_acc) <= 0 and j <=2:\n",
    "        j += 1\n",
    "        max_ppg = ppg_freqs[np.argsort(ppg_fft, axis = 0)[-j]]\n",
    "           \n",
    "    estimate_frequency = max_ppg \n",
    "    estimate.append(estimate_frequency * 60)\n",
    "        \n",
    "    window_Hz = 40/60\n",
    "    conf = np.sum(ppg_fft[(ppg_freqs >= (estimate_frequency - window_Hz)) & \n",
    "                        (ppg_freqs <= (estimate_frequency + window_Hz))]) / np.sum(ppg_fft)\n",
    "        \n",
    "    confidence.append(conf)\n",
    "        \n",
    "errors = np.abs(np.diag(estimate - reference_bpm))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Project Write-up\n",
    "\n",
    "Answer the following prompts to demonstrate understanding of the algorithm you wrote for this specific context.\n",
    "\n",
    "> - **Code Description** - Include details so someone unfamiliar with your project will know how to run your code and use your algorithm. \n",
    "> - **Data Description** - Describe the dataset that was used to train and test the algorithm. Include its short-comings and what data would be required to build a more complete dataset.\n",
    "> - **Algorithhm Description** will include the following:\n",
    ">   - how the algorithm works\n",
    ">   - the specific aspects of the physiology that it takes advantage of\n",
    ">   - a describtion of the algorithm outputs\n",
    ">   - caveats on algorithm outputs \n",
    ">   - common failure modes\n",
    "> - **Algorithm Performance** - Detail how performance was computed (eg. using cross-validation or train-test split) and what metrics were optimized for. Include error metrics that would be relevant to users of your algorithm. Caveat your performance numbers by acknowledging how generalizable they may or may not be on different datasets.\n",
    "\n",
    "Your write-up goes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Code Description:\n",
    "A part of the code was provided and shouldn;t be changed. Most of the student code goes into RunPulseRateAlgorithm. Some additional functions constituted as helper code to determine specific features. \n",
    "\n",
    "It basically calculates the heart pulse reate from provided PPG and accelorometer signals. The code runs the provided data against ground truth data where it calculates the heart rate and the confidence.\n",
    "\n",
    "All data is include in a data directory ./datasets/troika/training_data. \n",
    "\n",
    "Running the code is done by calling the function Evaluate().\n",
    "\n",
    "## Data Description\n",
    "The data from this project comes from the [Cardiac Arrythmia Suppression Trial (CAST)](https://physionet.org/content/crisdb/1.0.0/), which was sponsored by the National Heart, Lung, and Blood Institute (NHLBI). CAST collected 24 hours of heart rate data from ECGs from people who have had a myocardial infarction (MI) within the past two years.<sup>2</sup> This data has been smoothed and resampled to more closely resemble PPG-derived pulse rate data from a wrist wearable.<sup>3</sup>\n",
    "\n",
    "Two-channel PPG signals, three-axis acceleration signals, and one-channel ECG signals were simultaneously recorded from subjects with age from 18 to 35. For each subject, the PPG signals were recorded from wrist by two pulse oximeters with green LEDs (wavelength: 515nm). Their distance (from center to center) was 2 cm. The acceleration signal was also recorded from wrist by a three-axis accelerometer. Both the pulse oximeter and the accelerometer were embedded in a wristband, which was comfortably worn. The ECG signal was recorded simultaneously from the chest using wet ECG sensors. All signals were sampled at 125 Hz and sent to a nearby computer via Bluetooth.\n",
    "\n",
    "Each dataset with the similar name 'DATA_01_TYPE01' contains a variable 'sig'. It has 6 rows. The first row is a simultaneous recording of ECG, which is recorded from the chest of each subject. The second row and the third row are two channels of PPG, which are recorded from the wrist of each subject. The last three rows are simultaneous recordings of acceleration data (in x-, y-, and z-axis). \n",
    "\n",
    "During data recording, each subject ran on a treadmill with changing speeds. For datasets with names containing 'TYPE01', the running speeds changed as follows:\n",
    "  * rest(30s) -> 8km/h(1min) -> 15km/h(1min) -> 8km/h(1min) -> 15km/h(1min) -> rest(30s)\n",
    "\n",
    "For datasets with names containing 'TYPE02', the running speeds changed as follows:\n",
    "  * rest(30s) -> 6km/h(1min) -> 12km/h(1min) -> 6km/h(1min) -> 12km/h(1min) -> rest(30s)\n",
    "\n",
    "For each dataset with the similar name 'DATA_01_TYPE01', the ground-truth of heart rate can be calculated from the simultaneously recorded ECG signal (i.e. the first row of the variable 'sig'). For convenience, we also provide the calculated ground-truth heart rate, stored in the datasets with the corresponding name, say 'REF_01_TYPE01'. In each of this kind of datasets, there is a variable 'BPM0', which gives the BPM value in every 8-second time window. Note that two successive time windows overlap by 6 seconds. Thus the first value in 'BPM0' gives the calcualted heart rate ground-truth in the first 8 seconds, while the second value in 'BPM0' gives the calculated heart rate ground-truth from the 3rd second to the 10th second.\n",
    "\n",
    "\n",
    "## Algorithm Description\n",
    "1. Load the data from the training data folder \n",
    "2. Load the ground truth data from the same folder  \n",
    "3. The files are differentiated by name. Training data files start with 'data_*' whereas ground truth files start with 'ref_*'\n",
    "4. Some parameters, such as the windows length and windows shift must be set. Also it important to set the signal rate = 125. Bandpass is the paarameter to filter initial noisy data and is set to 40 to 240 BPM\n",
    "  * window_length = 8\n",
    "  * window_shift = 2\n",
    "  * fs = 125\n",
    "  * bandpasss = (40/60,240/60)\n",
    "5. run the BandPassFilter funtion on the loaded signal. It returns filtered data suchs a PPG and ACC magnitude. The ACC_magnitude is the mean of accx, accy, which are the 3 channels of accelorometer data in the x, y, z direction.\n",
    "6. The function Features, will featurize the bandpass data. The features include, Fourier Transform, Peak values, Frequecies and their specifc weigth. \n",
    "7. as last we estimate the pulse rate for the PPG signal and the acc (3 axis). The estimation is then compared with the ground truth. The result calculates the errors and confidence.\n",
    "\n",
    "## Algorithm performance\n",
    "The MAE (Mean ABsolute Error) estimates the heart rate in an 8 second window. Adjusting the frequency window parameter and running the algorithm in the Udacity workspace provided a passed result.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Next Steps\n",
    "You will now go to **Test Your Algorithm** (back in the Project Classroom) to apply a unit test to confirm that your algorithm met the success criteria. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}